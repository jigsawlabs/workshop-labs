{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsampling Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we were introduced to a random forest.  We saw that with decision trees, we not only get a high degree of variation in the leafs of a tree, but also throughout the tree.  This is because a decision tree works sequentially, a split of a data at a higher level affects how the data will be split at lower levels.  It's also because decision trees happen to have very few assumptions about the data -- unlike say a linear model which assumes that the data fits some form of a line.  With a decision trees,Â we are working with a very flexible model and this gives us a wide degree of variation.\n",
    "\n",
    "This variance is a problem if using just one decision tree to make our predictions.  However, we can reduce our variance by fitting many different trees, and then taking the average of the predictions.  This is the main idea behind a random forest.  In this lesson, we'll begin working with a random forest in sklearn.  As we move through this and subsequent lessons, it's important to remember that the main idea behind a random forest:\n",
    "\n",
    "* Use a highly flexible, and variant, model to find different patterns in the data and then reduce that variance through aggregation. \n",
    "\n",
    "Aggregating the models reduces the effects of random patterns which are unlikely to happen again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson, let's work with the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "dataset = load_diabetes()\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     X, y, test_size=0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest from the Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first idea of random forests is one that we already know.  With random forests we are averaging the predictions from a number of decision trees.  We can see this directly by loading up our random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=None,\n",
       "           oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators = 25, random_state = 1)\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we fit our random forest regressor, our regressors comes with a new attribute, called `estimators`.  An estimator is a particular tree of a random forest.  Let's take a look at the first two estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1791095845, splitter='best'),\n",
       " DecisionTreeRegressor(criterion='mse', max_depth=None, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=2135392491, splitter='best')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = rfr.estimators_\n",
    "len(estimators)\n",
    "# 25\n",
    "estimators[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each of these estimators is simply one of the decision trees that we saw earlier on.  We can see each of the estimator's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 83.,  93.,  47.,  77.,  96.,  69.,  69.,  69.,  93.,  64.,  96.,\n",
       "        88., 135.,  77.,  96.,  60.,  37.,  31., 116., 125.,  64.,  77.,\n",
       "        96.,  96.,  47.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tree_predictions = np.hstack([estimator.predict(X_test[:1, :]) for estimator in rfr.estimators_])\n",
    "tree_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to find the prediction of the random forest, we can just take the average of these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.04"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tree_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is exactly the prediction of our random forest regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([80.04])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.predict(X_test[:1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at the tree predictions, we'll see that we received different predictions on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{31.0, 64.0, 77.0, 78.0, 96.0, 116.0, 144.0, 146.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tree_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how would we receive ten predictions if we are training each estimator on exactly the same data, and each estimator is following the same rules?  The short answer is, we can't.  If we train ten estimators on exactly the same data, each tree would wind up exactly the same, and each would fit thus fit to the random variation in the data in the same way.  \n",
    "\n",
    "Fitting the same highly variant tree multiple times and then taking the average doesn't help us reduce our error of variance.  Instead we would like ten different trees, and then take the average of these differing trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish this, we simply train our tree on different samples of the data.  This idea is just what it sounds like.  Each tree of a random forest only trains on a subset of the training data.  This way, even if all of the features were trained on, we would get different trees.  This is because each tree would see different training data.\n",
    "\n",
    "So with a random forest we not only train multiple trees, but train the trees on different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we were introduced to a random forest.  We saw that we can initialize a random forest with a number of estimators, where each estimator is a decision tree that fits to our data.  To create variation in our decision trees, each decision tree is fit to a subset of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Random Forest Top to Bottom](https://www.gormanalysis.com/blog/random-forest-from-top-to-bottom/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
